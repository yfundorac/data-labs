<!DOCTYPE html>

<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Long short-term memory - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Long_short-term_memory","wgTitle":"Long short-term memory","wgCurRevisionId":916374525,"wgRevisionId":916374525,"wgArticleId":10711453,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from October 2017","Artificial neural networks"],"wgBreakFrames":!1,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Long_short-term_memory","wgRelevantArticleId":
10711453,"wgRequestId":"XaNYWwpAIC8AAKJvcMEAAAAA","wgCSPNonce":!1,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q6673524","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready",
"mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface",
"ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.35.0-wmf.1" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1200px-The_LSTM_cell.png" property="og:image"/>
<link href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Long_short-term_memory" rel="alternate"/>
<link href="/w/index.php?title=Long_short-term_memory&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Long_short-term_memory&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Long_short-term_memory rootpage-Long_short-term_memory skin-vector action-view">
<div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Long short-term memory</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a class="image" href="/wiki/File:Kernel_Machine.svg"><img alt="Kernel Machine.svg" data-file-height="233" data-file-width="512" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" width="220"/></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a class="mw-redirect" href="/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a class="mw-redirect" href="/wiki/Artificial_neural_networks" title="Artificial neural networks">Artificial neural networks</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a class="mw-selflink selflink">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="thumb tright"><div class="thumbinner" style="width:172px;"><a class="image" href="/wiki/File:The_LSTM_cell.png"><img alt="" class="thumbimage" data-file-height="1322" data-file-width="2014" decoding="async" height="112" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/170px-The_LSTM_cell.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/255px-The_LSTM_cell.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/340px-The_LSTM_cell.png 2x" width="170"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:The_LSTM_cell.png" title="Enlarge"></a></div>The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.</div></div></div>
<p><b>Long short-term memory</b> (<b>LSTM</b>) is an artificial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) architecture<sup class="reference" id="cite_ref-lstm1997_1-0"><a href="#cite_note-lstm1997-1">[1]</a></sup> used in the field of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>. Unlike standard <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a><sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<sup class="reference" id="cite_ref-sak2014_3-0"><a href="#cite_note-sak2014-3">[3]</a></sup><sup class="reference" id="cite_ref-liwu2015_4-0"><a href="#cite_note-liwu2015-4">[4]</a></sup>
<a class="mw-redirect" href="/wiki/Bloomberg_Business_Week" title="Bloomberg Business Week">Bloomberg Business Week</a> wrote: "These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music."<sup class="reference" id="cite_ref-bloomberg2018_5-0"><a href="#cite_note-bloomberg2018-5">[5]</a></sup>
</p><p>A common LSTM unit is composed of a <b>cell</b>, an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. The cell remembers values over arbitrary time intervals and the three <i>gates</i> regulate the flow of information into and out of the cell.
</p><p>LSTM networks are well-suited to <a class="mw-redirect" href="/wiki/Classification_in_machine_learning" title="Classification in machine learning">classifying</a>, <a class="mw-redirect" href="/wiki/Computer_data_processing" title="Computer data processing">processing</a> and <a class="mw-redirect" href="/wiki/Predict" title="Predict">making predictions</a> based on <a href="/wiki/Time_series" title="Time series">time series</a> data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing</a> gradient problems that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, <a class="mw-redirect" href="/wiki/Hidden_Markov_models" title="Hidden Markov models">hidden Markov models</a> and other sequence learning methods in numerous applications.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>
</p>
<div class="toc" id="toc"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Idea"><span class="tocnumber">2</span> <span class="toctext">Idea</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Architecture"><span class="tocnumber">3</span> <span class="toctext">Architecture</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Variants"><span class="tocnumber">4</span> <span class="toctext">Variants</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#LSTM_with_a_forget_gate"><span class="tocnumber">4.1</span> <span class="toctext">LSTM with a forget gate</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Variables"><span class="tocnumber">4.1.1</span> <span class="toctext">Variables</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Activation_functions"><span class="tocnumber">4.1.2</span> <span class="toctext">Activation functions</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Peephole_LSTM"><span class="tocnumber">4.2</span> <span class="toctext">Peephole LSTM</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Peephole_convolutional_LSTM"><span class="tocnumber">4.3</span> <span class="toctext">Peephole convolutional LSTM</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Training"><span class="tocnumber">5</span> <span class="toctext">Training</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#CTC_score_function"><span class="tocnumber">5.1</span> <span class="toctext">CTC score function</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Alternatives"><span class="tocnumber">5.2</span> <span class="toctext">Alternatives</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Success"><span class="tocnumber">5.2.1</span> <span class="toctext">Success</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Applications"><span class="tocnumber">6</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>LSTM was proposed in 1997 by <a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Sepp Hochreiter</a> and <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>.<sup class="reference" id="cite_ref-lstm1997_1-1"><a href="#cite_note-lstm1997-1">[1]</a></sup> By introducing Constant Error Carousel (CEC) units, LSTM deals with the exploding and vanishing gradient problems. The initial version of LSTM block included cells, input and output gates.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_6-0"><a href="#cite_note-ASearchSpaceOdyssey-6">[6]</a></sup>
</p><p>In 1999, <a href="/wiki/Felix_Gers" title="Felix Gers">Felix Gers</a> and his advisor <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a> and Fred Cummins introduced the forget gate (also called “keep gate”) into LSTM architecture,<sup class="reference" id="cite_ref-lstm1999_7-0"><a href="#cite_note-lstm1999-7">[7]</a></sup> 
enabling the LSTM to reset its own state.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_6-1"><a href="#cite_note-ASearchSpaceOdyssey-6">[6]</a></sup>
</p><p>In 2000, Gers &amp; Schmidhuber &amp; Cummins added peephole connections (connections from the cell to the gates) into the architecture.<sup class="reference" id="cite_ref-lstm2000_8-0"><a href="#cite_note-lstm2000-8">[8]</a></sup> Additionally, the output activation function was omitted.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_6-2"><a href="#cite_note-ASearchSpaceOdyssey-6">[6]</a></sup>
</p><p>In 2014, Kyunghyun Cho et al. put forward a simplified variant called <a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit</a> (GRU).<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>
</p><p>Among other successes, LSTM achieved record results in natural language text compression,<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> unsegmented connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a><sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> and won the <a class="mw-redirect" href="/wiki/ICDAR" title="ICDAR">ICDAR</a> handwriting competition (2009). LSTM networks were a major component of a network that achieved a record 17.7% <a href="/wiki/Phoneme" title="Phoneme">phoneme</a> error rate on the classic <a href="/wiki/TIMIT" title="TIMIT">TIMIT</a> natural speech dataset (2013).<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup>
</p><p>As of 2016, major technology companies including <a href="/wiki/Google" title="Google">Google</a>, <a href="/wiki/Apple_Inc." title="Apple Inc.">Apple</a>, and <a href="/wiki/Microsoft" title="Microsoft">Microsoft</a> were using LSTM as fundamental components in new products.<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> For example, Google used LSTM for speech recognition on the <a href="/wiki/Smartphone" title="Smartphone">smartphone</a>,<sup class="reference" id="cite_ref-Beau15_14-0"><a href="#cite_note-Beau15-14">[14]</a></sup><sup class="reference" id="cite_ref-GoogleVoiceSearch_15-0"><a href="#cite_note-GoogleVoiceSearch-15">[15]</a></sup> for the smart assistant Allo<sup class="reference" id="cite_ref-GoogleAllo_16-0"><a href="#cite_note-GoogleAllo-16">[16]</a></sup> and for <a href="/wiki/Google_Translate" title="Google Translate">Google Translate</a>.<sup class="reference" id="cite_ref-GoogleTranslate_17-0"><a href="#cite_note-GoogleTranslate-17">[17]</a></sup><sup class="reference" id="cite_ref-WiredGoogleTranslate_18-0"><a href="#cite_note-WiredGoogleTranslate-18">[18]</a></sup> <a href="/wiki/Apple_Inc." title="Apple Inc.">Apple</a> uses LSTM for the "Quicktype" function on the <a href="/wiki/IPhone" title="IPhone">iPhone</a><sup class="reference" id="cite_ref-AppleQuicktype_19-0"><a href="#cite_note-AppleQuicktype-19">[19]</a></sup><sup class="reference" id="cite_ref-AppleQuicktype2_20-0"><a href="#cite_note-AppleQuicktype2-20">[20]</a></sup> and for <a href="/wiki/Siri" title="Siri">Siri</a>.<sup class="reference" id="cite_ref-AppleSiri_21-0"><a href="#cite_note-AppleSiri-21">[21]</a></sup> <a class="mw-redirect" href="/wiki/Amazon_Inc." title="Amazon Inc.">Amazon</a> uses LSTM for <a href="/wiki/Amazon_Alexa" title="Amazon Alexa">Amazon Alexa</a>.<sup class="reference" id="cite_ref-AmazonAlexa_22-0"><a href="#cite_note-AmazonAlexa-22">[22]</a></sup>
</p><p>In 2017, Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.<sup class="reference" id="cite_ref-FacebookTranslate_23-0"><a href="#cite_note-FacebookTranslate-23">[23]</a></sup>
</p><p>In 2017, researchers from <a href="/wiki/Michigan_State_University" title="Michigan State University">Michigan State University</a>, <a href="/wiki/IBM_Research" title="IBM Research">IBM Research</a>, and <a href="/wiki/Cornell_University" title="Cornell University">Cornell University</a> published a study in the Knowledge Discovery and Data Mining (KDD) conference.<sup class="reference" id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup><sup class="reference" id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup><sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> Their study describes a novel neural network that performs better on certain data sets than the widely used long short-term memory neural network.
</p><p>Further in 2017 Microsoft reported reaching 95.1% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used "dialog session-based long-short-term memory".<sup class="reference" id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup>
</p>
<h2><span class="mw-headline" id="Idea">Idea</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=2" title="Edit section: Idea">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In theory, classic (or "vanilla") <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNNs</a> can keep track of arbitrary long-term dependencies in the input sequences. The problem of vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using <a class="mw-redirect" href="/wiki/Back-propagation" title="Back-propagation">back-propagation</a>, the gradients which are back-propagated can <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">"vanish" (that is, they can tend to zero) or "explode" (that is, they can tend to infinity)</a>, because of the computations involved in the process, which use <a href="/wiki/Round-off_error" title="Round-off error">finite-precision numbers</a>. RNNs using LSTM units partially solve the <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing gradient problem</a>, because LSTM units allow gradients to also flow <i>unchanged</i>. However, LSTM networks can still suffer from the exploding gradient problem.<sup class="reference" id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup>
</p>
<h2><span class="mw-headline" id="Architecture">Architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=3" title="Edit section: Architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are several architectures of LSTM units. A common architecture is composed of a <b>cell</b> (the memory part of the LSTM unit) and three "regulators", usually called gates, of the flow of information inside the LSTM unit: an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. Some variations of the LSTM unit do not have one or more of these gates or maybe have other gates. For example, <a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">gated recurrent units</a> (GRUs) do not have an output gate.
</p><p>Intuitively, the <i>cell</i> is responsible for keeping track of the dependencies between the elements in the input sequence. The <i>input gate</i> controls the extent to which a new value flows into the cell, the <i>forget gate</i> controls the extent to which a value remains in the cell and the <i>output gate</i> controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. The activation function of the LSTM <i>gates</i> is often the <a class="mw-redirect" href="/wiki/Logistic_sigmoid_function" title="Logistic sigmoid function">logistic sigmoid function</a>.
</p><p>There are connections into and out of the LSTM <i>gates</i>, a few of which are recurrent. The weights of these connections, which need to be learned during <a href="/wiki/Supervised_learning" title="Supervised learning">training</a>, determine how the gates operate.
</p>
<h2><span class="mw-headline" id="Variants">Variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=4" title="Edit section: Variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In the equations below, the lowercase variables represent vectors. Matrices <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W_{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W_{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle W_{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d16355ad959593cf720b24fffe62d99af53d15d9" style="vertical-align: -1.005ex; width:3.182ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle U_{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle U_{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle U_{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/05e27486afb11613504d6d6b9f6bd72e322607c8" style="vertical-align: -1.005ex; width:2.576ex; height:2.843ex;"/></span> contain, respectively, the weights of the input and recurrent connections, where the subscript <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle _{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle _{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle _{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e333a711146f91533eb5a030accc0e90948e4f92" style="vertical-align: -1.005ex; width:0.989ex; height:1.676ex;"/></span> can either be the input gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i}</annotation>
</semantics>
</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>, output gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o}</annotation>
</semantics>
</math></span><img alt="o" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c1031f61947aa3d1cf3a70ec3e4904df2c3675d" style="vertical-align: -0.338ex; width:1.128ex; height:1.676ex;"/></span>, the forget gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> or the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span>, depending on the activation being calculated. In this section, we are thus using a "vector notation". So, for example, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d6f2af820422ed59a0f14af91eee7498ebc4a2" style="vertical-align: -0.671ex; width:7.531ex; height:3.009ex;"/></span> is not just one cell of one LSTM unit, but contains <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>h</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h}</annotation>
</semantics>
</math></span><img alt="h" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a" style="vertical-align: -0.338ex; width:1.339ex; height:2.176ex;"/></span> LSTM unit's cells.
</p>
<h3><span class="mw-headline" id="LSTM_with_a_forget_gate">LSTM with a forget gate</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=5" title="Edit section: LSTM with a forget gate">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The compact forms of the equations for the forward pass of an LSTM unit with a forget gate are:<sup class="reference" id="cite_ref-lstm1997_1-2"><a href="#cite_note-lstm1997-1">[1]</a></sup><sup class="reference" id="cite_ref-lstm2000_8-1"><a href="#cite_note-lstm2000-8">[8]</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1" style="vertical-align: -7.338ex; width:44.566ex; height:15.843ex;"/></span></dd></dl>
<p>where the initial values are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{0}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{0}=0}</annotation>
</semantics>
</math></span><img alt="c_{0}=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29af3d4e887815bb3b9b9eab4f7540a376fccd73" style="vertical-align: -0.671ex; width:6.322ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{0}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{0}=0}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{0}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14a294b6cf9cbde4c37efd966913a63d316e615c" style="vertical-align: -0.671ex; width:6.654ex; height:2.509ex;"/></span> and the operator <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \circ }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>∘<!-- ∘ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \circ }</annotation>
</semantics>
</math></span><img alt="\circ " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99add39d2b681e2de7ff62422c32704a05c7ec31" style="vertical-align: 0.125ex; margin-bottom: -0.297ex; width:1.162ex; height:1.509ex;"/></span> denotes the <a href="/wiki/Hadamard_product_(matrices)" title="Hadamard product (matrices)">Hadamard product</a> (element-wise product). The subscript <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> indexes the time step.
</p>
<h4><span class="mw-headline" id="Variables">Variables</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=6" title="Edit section: Variables">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{t}\in \mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{t}\in \mathbb {R} ^{d}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle x_{t}\in \mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d528d57c5517e90795e0a6d6760463564236fee7" style="vertical-align: -0.671ex; width:7.766ex; height:3.009ex;"/></span>: input vector to the LSTM unit</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle f_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/02547735a64d79c553b23eaf3aeaaaf2fcde6eba" style="vertical-align: -0.671ex; width:7.663ex; height:3.009ex;"/></span>: forget gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/abb830b2d64edaa2aa92edaeccdcb027afa7344e" style="vertical-align: -0.671ex; width:7.326ex; height:3.009ex;"/></span>: input/update gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle o_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/25b8c014b13f62fbe82f9a277a71f6a10c2ae330" style="vertical-align: -0.671ex; width:7.651ex; height:3.009ex;"/></span>: output gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf6b05b4bd0106b70d036400f1ddd3ae54be2689" style="vertical-align: -0.671ex; width:7.863ex; height:3.009ex;"/></span>: hidden state vector also known as output vector of the LSTM unit</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d6f2af820422ed59a0f14af91eee7498ebc4a2" style="vertical-align: -0.671ex; width:7.531ex; height:3.009ex;"/></span>: cell state vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W\in \mathbb {R} ^{h\times d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
<mo>×<!-- × --></mo>
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W\in \mathbb {R} ^{h\times d}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle W\in \mathbb {R} ^{h\times d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/925afdb42f13f1d912db87ecda65135eb9fe6352" style="vertical-align: -0.338ex; width:10.271ex; height:2.676ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle U\in \mathbb {R} ^{h\times h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>U</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
<mo>×<!-- × --></mo>
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle U\in \mathbb {R} ^{h\times h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle U\in \mathbb {R} ^{h\times h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ff9bb53a5409a1e51f6130b3dfbcdad63324880" style="vertical-align: -0.338ex; width:9.706ex; height:2.676ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle b\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>b</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle b\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle b\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/289515b23d7df7e2e09f2ee38951abf345e60080" style="vertical-align: -0.338ex; width:6.695ex; height:2.676ex;"/></span>: weight matrices and bias vector parameters which need to be learned during training</li></ul>
<p>where the superscripts <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle d}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>d</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle d}</annotation>
</semantics>
</math></span><img alt="d" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab" style="vertical-align: -0.338ex; width:1.216ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>h</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h}</annotation>
</semantics>
</math></span><img alt="h" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a" style="vertical-align: -0.338ex; width:1.339ex; height:2.176ex;"/></span> refer to the number of input features and number of hidden units, respectively.
</p>
<h4><span class="mw-headline" id="Activation_functions"><a href="/wiki/Activation_function" title="Activation function">Activation functions</a></span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=7" title="Edit section: Activation functions">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{g}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{g}}</annotation>
</semantics>
</math></span><img alt="\sigma _{g}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/086f92de077f853afd7f5d22fb3d305cbf5e0ac3" style="vertical-align: -1.005ex; width:2.349ex; height:2.343ex;"/></span>: <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a>.</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{c}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{c}}</annotation>
</semantics>
</math></span><img alt="\sigma_c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b436b43abda74fce1a6859e03d34c914c6a240f4" style="vertical-align: -0.671ex; width:2.272ex; height:2.009ex;"/></span>: <a class="mw-redirect" href="/wiki/Hyperbolic_tangent" title="Hyperbolic tangent">hyperbolic tangent</a> function.</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{h}}</annotation>
</semantics>
</math></span><img alt="\sigma _{h}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8a7f19495f5a65d26570b54b7ca332956d27b27b" style="vertical-align: -0.671ex; width:2.506ex; height:2.009ex;"/></span>: hyperbolic tangent function or, as the peephole LSTM paper<sup class="reference" id="cite_ref-peepholeLSTM_29-0"><a href="#cite_note-peepholeLSTM-29">[29]</a></sup><sup class="reference" id="cite_ref-peephole2002_30-0"><a href="#cite_note-peephole2002-30">[30]</a></sup> suggests, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{h}(x)=x}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mi>x</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{h}(x)=x}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \sigma _{h}(x)=x}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/98d34299f4e04f10f7c22e1219bf182712c3e0fc" style="vertical-align: -0.838ex; width:10.074ex; height:2.843ex;"/></span>.</li></ul>
<h3><span class="mw-headline" id="Peephole_LSTM">Peephole LSTM</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=8" title="Edit section: Peephole LSTM">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Peephole_Long_Short-Term_Memory.svg"><img alt="" class="thumbimage" data-file-height="298" data-file-width="542" decoding="async" height="165" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/300px-Peephole_Long_Short-Term_Memory.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/450px-Peephole_Long_Short-Term_Memory.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/600px-Peephole_Long_Short-Term_Memory.svg.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Peephole_Long_Short-Term_Memory.svg" title="Enlarge"></a></div>A <a href="#Peephole_LSTM">peephole LSTM</a> unit with input (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i}</annotation>
</semantics>
</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>), output (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o}</annotation>
</semantics>
</math></span><img alt="o" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c1031f61947aa3d1cf3a70ec3e4904df2c3675d" style="vertical-align: -0.338ex; width:1.128ex; height:1.676ex;"/></span>), and forget (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span>) gates. Each of these gates can be thought as a "standard" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t},o_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t},o_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf0b0dee7b12fd921a114101ff11c83e1606a1f8" style="vertical-align: -0.671ex; width:4.616ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}}</annotation>
</semantics>
</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span> represent the activations of respectively the input, output and forget gates, at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span>.  The 3 exit arrows from the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> to the 3 gates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i,o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>,</mo>
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i,o}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i,o}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4697b39f565cd54942b9f81d5de46dcdd1174528" style="vertical-align: -0.671ex; width:2.964ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> represent the <i>peephole</i> connections. These peephole connections actually denote the contributions of the activation of the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t-1}</annotation>
</semantics>
</math></span><img alt="t-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a215d9553945bb84b3b5a79cc796fb7d6e0629f0" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>, i.e. the contribution of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span> (and not <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93578e37f3234419a34df79845836bc0ec5ef76c" style="vertical-align: -0.671ex; width:1.833ex; height:2.009ex;"/></span>, as the picture may suggest). In other words, the gates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i,o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>,</mo>
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i,o}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i,o}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4697b39f565cd54942b9f81d5de46dcdd1174528" style="vertical-align: -0.671ex; width:2.964ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> calculate their activations at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> (i.e., respectively, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t},o_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t},o_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf0b0dee7b12fd921a114101ff11c83e1606a1f8" style="vertical-align: -0.671ex; width:4.616ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}}</annotation>
</semantics>
</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span>) also considering the activation of the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t-1}</annotation>
</semantics>
</math></span><img alt="t-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a215d9553945bb84b3b5a79cc796fb7d6e0629f0" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>, i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span>.  The single left-to-right arrow exiting the memory cell is <i>not</i> a peephole connection and denotes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93578e37f3234419a34df79845836bc0ec5ef76c" style="vertical-align: -0.671ex; width:1.833ex; height:2.009ex;"/></span>.  The little circles containing a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \times }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>×<!-- × --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \times }</annotation>
</semantics>
</math></span><img alt="\times " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ffafff1ad26cbe49045f19a67ce532116a32703" style="vertical-align: 0.019ex; margin-bottom: -0.19ex; width:1.808ex; height:1.509ex;"/></span> symbol represent an element-wise multiplication between its inputs. The big circles containing an <i>S</i>-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.  There are many other kinds of LSTMs as well.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_6-3"><a href="#cite_note-ASearchSpaceOdyssey-6">[6]</a></sup></div></div></div>
<p>The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM).<sup class="reference" id="cite_ref-peepholeLSTM_29-1"><a href="#cite_note-peepholeLSTM-29">[29]</a></sup><sup class="reference" id="cite_ref-peephole2002_30-1"><a href="#cite_note-peephole2002-30">[30]</a></sup> Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.<sup class="reference" id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf56fc7e1114417475762546403f3d66460975d0" style="vertical-align: -0.671ex; width:4.265ex; height:2.509ex;"/></span> is not used, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span> is used instead in most places.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&amp;=\sigma _{h}(o_{t}\circ c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&amp;=\sigma _{h}(o_{t}\circ c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&amp;=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&amp;=\sigma _{h}(o_{t}\circ c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d144d712970f336e7f91fcdd78dd1b8946c54996" style="vertical-align: -7.338ex; width:34.928ex; height:15.843ex;"/></span></dd></dl>
<h3><span class="mw-headline" id="Peephole_convolutional_LSTM">Peephole convolutional LSTM</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=9" title="Edit section: Peephole convolutional LSTM">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Peephole <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional</a> LSTM.<sup class="reference" id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup> The <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle *}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>∗<!-- ∗ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle *}</annotation>
</semantics>
</math></span><img alt="*" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8e9972f426d9e07855984f73ee195a21dbc21755" style="vertical-align: 0.079ex; margin-bottom: -0.25ex; width:1.162ex; height:1.509ex;"/></span> denotes the <a href="/wiki/Convolution" title="Convolution">convolution</a> operator.
</p>
<dl><dd><span class="mwe-math-element" id="Page 4, formula 4 in [33] reference (Ot is calculated for C(t) intead of C(t-1)): https://arxiv.org/abs/1506.04214v2"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&amp;=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&amp;=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&amp;=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&amp;=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&amp;=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&amp;=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&amp;=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" id="Page_4,_formula_4_in_[33]_reference_(Ot_is_calculated_for_C(t)_intead_of_C(t-1)):_https://arxiv.org/abs/1506.04214v2" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1edbece2559479959fe829e9c6657efb380debe7" style="vertical-align: -7.338ex; width:48.955ex; height:15.843ex;"/></span></dd></dl>
<h2><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=10" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An RNN using LSTM units can be trained in a supervised fashion, on a set of training sequences, using an optimization algorithm, like <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>, combined with <a href="/wiki/Backpropagation_through_time" title="Backpropagation through time">backpropagation through time</a> to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
</p><p>A problem with using <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> for standard RNNs is that error gradients <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanish</a> exponentially quickly with the size of the time lag between important events. This is due to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \lim _{n\to \infty }W^{n}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munder>
<mo form="prefix" movablelimits="true">lim</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</munder>
<msup>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msup>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \lim _{n\to \infty }W^{n}=0}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \lim _{n\to \infty }W^{n}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f21d24f36ac54c2e3826fe618891ce17b19e12d" style="vertical-align: -1.838ex; width:12.647ex; height:3.843ex;"/></span> if the <a href="/wiki/Spectral_radius" title="Spectral radius">spectral radius</a> of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W}</annotation>
</semantics>
</math></span><img alt="W" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" style="vertical-align: -0.338ex; width:2.435ex; height:2.176ex;"/></span> is smaller than 1.<sup class="reference" id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup><sup class="reference" id="cite_ref-gradf_34-0"><a href="#cite_note-gradf-34">[34]</a></sup>
</p><p>However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This "error carousel" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.
</p>
<h3><span class="mw-headline" id="CTC_score_function">CTC score function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=11" title="Edit section: CTC score function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many applications use stacks of LSTM RNNs<sup class="reference" id="cite_ref-fernandez2007_35-0"><a href="#cite_note-fernandez2007-35">[35]</a></sup> and train them by <a class="mw-redirect" href="/wiki/Connectionist_temporal_classification_(CTC)" title="Connectionist temporal classification (CTC)">connectionist temporal classification (CTC)</a><sup class="reference" id="cite_ref-graves2006_36-0"><a href="#cite_note-graves2006-36">[36]</a></sup> to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
</p>
<h3><span class="mw-headline" id="Alternatives">Alternatives</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=12" title="Edit section: Alternatives">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Sometimes, it can be advantageous to train (parts of) an LSTM by <a href="/wiki/Neuroevolution" title="Neuroevolution">neuroevolution</a><sup class="reference" id="cite_ref-wierstra2005_37-0"><a href="#cite_note-wierstra2005-37">[37]</a></sup> or by policy gradient methods, especially when there is no "teacher" (that is, training labels).
</p>
<h4><span class="mw-headline" id="Success">Success</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=13" title="Edit section: Success">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.
</p><p>In 2018, <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a> called it a “huge milestone in advancing artificial intelligence” when bots developed by <a href="/wiki/OpenAI" title="OpenAI">OpenAI</a> were able to beat humans in the game of Dota 2.<sup class="reference" id="cite_ref-OpenAIfive_38-0"><a href="#cite_note-OpenAIfive-38">[38]</a></sup> OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.<sup class="reference" id="cite_ref-OpenAIfive_38-1"><a href="#cite_note-OpenAIfive-38">[38]</a></sup>
</p><p>In 2018, <a href="/wiki/OpenAI" title="OpenAI">OpenAI</a> also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.<sup class="reference" id="cite_ref-OpenAIhand_39-0"><a href="#cite_note-OpenAIhand-39">[39]</a></sup>
</p><p>In 2019, <a href="/wiki/DeepMind" title="DeepMind">DeepMind</a>'s program AlphaStar used a deep LSTM core to excel at the complex video game <a class="mw-redirect" href="/wiki/Starcraft" title="Starcraft">Starcraft</a>.<sup class="reference" id="cite_ref-alphastar_40-0"><a href="#cite_note-alphastar-40">[40]</a></sup> This was viewed as significant progress towards Artificial General Intelligence.<sup class="reference" id="cite_ref-alphastar_40-1"><a href="#cite_note-alphastar-40">[40]</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=14" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Applications of LSTM include:
</p>
<ul><li><a href="/wiki/Robot_control" title="Robot control">Robot control</a><sup class="reference" id="cite_ref-41"><a href="#cite_note-41">[41]</a></sup></li>
<li><a class="mw-redirect" href="/wiki/Time_series_prediction" title="Time series prediction">Time series prediction</a><sup class="reference" id="cite_ref-wierstra2005_37-1"><a href="#cite_note-wierstra2005-37">[37]</a></sup></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a><sup class="reference" id="cite_ref-42"><a href="#cite_note-42">[42]</a></sup><sup class="reference" id="cite_ref-43"><a href="#cite_note-43">[43]</a></sup><sup class="reference" id="cite_ref-ReferenceA_44-0"><a href="#cite_note-ReferenceA-44">[44]</a></sup></li>
<li>Rhythm learning<sup class="reference" id="cite_ref-peephole2002_30-2"><a href="#cite_note-peephole2002-30">[30]</a></sup></li>
<li>Music composition<sup class="reference" id="cite_ref-45"><a href="#cite_note-45">[45]</a></sup></li>
<li>Grammar learning<sup class="reference" id="cite_ref-46"><a href="#cite_note-46">[46]</a></sup><sup class="reference" id="cite_ref-peepholeLSTM_29-2"><a href="#cite_note-peepholeLSTM-29">[29]</a></sup><sup class="reference" id="cite_ref-47"><a href="#cite_note-47">[47]</a></sup></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">Handwriting recognition</a><sup class="reference" id="cite_ref-48"><a href="#cite_note-48">[48]</a></sup><sup class="reference" id="cite_ref-49"><a href="#cite_note-49">[49]</a></sup></li>
<li>Human action recognition<sup class="reference" id="cite_ref-50"><a href="#cite_note-50">[50]</a></sup></li>
<li><a href="/wiki/Sign_language" title="Sign language">Sign Language Translation</a><sup class="reference" id="cite_ref-51"><a href="#cite_note-51">[51]</a></sup></li>
<li>Protein Homology Detection<sup class="reference" id="cite_ref-52"><a href="#cite_note-52">[52]</a></sup></li>
<li>Predicting subcellular localization of proteins<sup class="reference" id="cite_ref-53"><a href="#cite_note-53">[53]</a></sup></li>
<li>Time series anomaly detection<sup class="reference" id="cite_ref-54"><a href="#cite_note-54">[54]</a></sup></li>
<li>Several prediction tasks in the area of business process management<sup class="reference" id="cite_ref-55"><a href="#cite_note-55">[55]</a></sup></li>
<li>Prediction in medical care pathways<sup class="reference" id="cite_ref-56"><a href="#cite_note-56">[56]</a></sup></li>
<li><a href="/wiki/Semantic_parsing" title="Semantic parsing">Semantic parsing</a><sup class="reference" id="cite_ref-57"><a href="#cite_note-57">[57]</a></sup></li>
<li><a href="/wiki/Object_Co-segmentation" title="Object Co-segmentation">Object Co-segmentation</a><sup class="reference" id="cite_ref-Wang_Duan_Zhang_Niu_p=1657_58-0"><a href="#cite_note-Wang_Duan_Zhang_Niu_p=1657-58">[58]</a></sup><sup class="reference" id="cite_ref-Duan_Wang_Zhai_Zheng_2018_p._59-0"><a href="#cite_note-Duan_Wang_Zhai_Zheng_2018_p.-59">[59]</a></sup></li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=15" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Long-term_potentiation" title="Long-term potentiation">Long-term potentiation</a></li>
<li><a href="/wiki/Prefrontal_cortex_basal_ganglia_working_memory" title="Prefrontal cortex basal ganglia working memory">Prefrontal cortex basal ganglia working memory</a></li>
<li><a href="/wiki/Time_series" title="Time series">Time series</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=16" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-lstm1997-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-lstm1997_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lstm1997_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lstm1997_1-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Sepp Hochreiter</a>; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a> (1997). <a class="external text" href="https://www.researchgate.net/publication/13853244" rel="nofollow">"Long short-term memory"</a>. <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>9</b> (8): 1735–1780. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1162%2Fneco.1997.9.8.1735" rel="nofollow">10.1162/neco.1997.9.8.1735</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/9377276" rel="nofollow">9377276</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Long+short-term+memory&amp;rft.volume=9&amp;rft.issue=8&amp;rft.pages=1735-1780&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1997.9.8.1735&amp;rft_id=info%3Apmid%2F9377276&amp;rft.au=Sepp+Hochreiter&amp;rft.au=J%C3%BCrgen+Schmidhuber&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F13853244&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, J.</a> (2009). <a class="external text" href="http://www.idsia.ch/~juergen/tpami_2008.pdf" rel="nofollow">"A Novel Connectionist System for Improved Unconstrained Handwriting Recognition"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502" rel="nofollow">10.1.1.139.4502</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Ftpami.2008.137" rel="nofollow">10.1109/tpami.2008.137</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19299860" rel="nofollow">19299860</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=A+Novel+Connectionist+System+for+Improved+Unconstrained+Handwriting+Recognition&amp;rft.volume=31&amp;rft.issue=5&amp;rft.pages=855-868&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&amp;rft_id=info%3Apmid%2F19299860&amp;rft_id=info%3Adoi%2F10.1109%2Ftpami.2008.137&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Liwicki%2C+M.&amp;rft.au=Fernandez%2C+S.&amp;rft.au=Bertolami%2C+R.&amp;rft.au=Bunke%2C+H.&amp;rft.au=Schmidhuber%2C+J.&amp;rft_id=http%3A%2F%2Fwww.idsia.ch%2F~juergen%2Ftpami_2008.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-sak2014-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-sak2014_3-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). <a class="external text" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf" rel="nofollow">"Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling"</a> <span class="cs1-format">(PDF)</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Long+Short-Term+Memory+recurrent+neural+network+architectures+for+large+scale+acoustic+modeling&amp;rft.date=2014&amp;rft.aulast=Sak&amp;rft.aufirst=Hasim&amp;rft.au=Senior%2C+Andrew&amp;rft.au=Beaufays%2C+Francoise&amp;rft_id=https%3A%2F%2Fstatic.googleusercontent.com%2Fmedia%2Fresearch.google.com%2Fen%2F%2Fpubs%2Farchive%2F43905.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-liwu2015-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-liwu2015_4-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Li, Xiangang; Wu, Xihong (2014-10-15). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1410.4281" rel="nofollow">1410.4281</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Constructing+Long+Short-Term+Memory+based+Deep+Recurrent+Neural+Networks+for+Large+Vocabulary+Speech+Recognition&amp;rft.date=2014-10-15&amp;rft_id=info%3Aarxiv%2F1410.4281&amp;rft.aulast=Li&amp;rft.aufirst=Xiangang&amp;rft.au=Wu%2C+Xihong&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-bloomberg2018-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-bloomberg2018_5-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Vance, Ashlee (May 15, 2018). <a class="external text" href="https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune" rel="nofollow">"Quote: These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music"</a>. <i>Bloomberg Business Week</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-16</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bloomberg+Business+Week&amp;rft.atitle=Quote%3A+These+powers+make+LSTM+arguably+the+most+commercial+AI+achievement%2C+used+for+everything+from+predicting+diseases+to+composing+music.&amp;rft.date=2018-05-15&amp;rft.aulast=Vance&amp;rft.aufirst=Ashlee&amp;rft_id=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Ffeatures%2F2018-05-15%2Fgoogle-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ASearchSpaceOdyssey-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-ASearchSpaceOdyssey_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_6-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Klaus Greff; Rupesh Kumar Srivastava; Jan Koutník; Bas R. Steunebrink; Jürgen Schmidhuber (2015). "LSTM: A Search Space Odyssey". <i>IEEE Transactions on Neural Networks and Learning Systems</i>. <b>28</b> (10): 2222–2232. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1503.04069" rel="nofollow">1503.04069</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv150304069G" rel="nofollow">2015arXiv150304069G</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2FTNNLS.2016.2582924" rel="nofollow">10.1109/TNNLS.2016.2582924</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/27411231" rel="nofollow">27411231</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks+and+Learning+Systems&amp;rft.atitle=LSTM%3A+A+Search+Space+Odyssey&amp;rft.volume=28&amp;rft.issue=10&amp;rft.pages=2222-2232&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1503.04069&amp;rft_id=info%3Apmid%2F27411231&amp;rft_id=info%3Adoi%2F10.1109%2FTNNLS.2016.2582924&amp;rft_id=info%3Abibcode%2F2015arXiv150304069G&amp;rft.au=Klaus+Greff&amp;rft.au=Rupesh+Kumar+Srivastava&amp;rft.au=Jan+Koutn%C3%ADk&amp;rft.au=Bas+R.+Steunebrink&amp;rft.au=J%C3%BCrgen+Schmidhuber&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lstm1999-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-lstm1999_7-0">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Felix_Gers" title="Felix Gers">Felix Gers</a>; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>; Fred Cummins (1999). <a class="external text" href="https://ieeexplore.ieee.org/document/818041" rel="nofollow">"Learning to Forget: Continual Prediction with LSTM"</a>. <i>Proc. ICANN'99, IEE, London</i>: 850–855.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+ICANN%2799%2C+IEE%2C+London&amp;rft.atitle=Learning+to+Forget%3A+Continual+Prediction+with+LSTM&amp;rft.pages=850-855&amp;rft.date=1999&amp;rft.au=Felix+Gers&amp;rft.au=J%C3%BCrgen+Schmidhuber&amp;rft.au=Fred+Cummins&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F818041&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lstm2000-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-lstm2000_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lstm2000_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>12</b> (10): 2451–2471. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709" rel="nofollow">10.1.1.55.5709</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1162%2F089976600300015015" rel="nofollow">10.1162/089976600300015015</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Learning+to+Forget%3A+Continual+Prediction+with+LSTM&amp;rft.volume=12&amp;rft.issue=10&amp;rft.pages=2451-2471&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.55.5709&amp;rft_id=info%3Adoi%2F10.1162%2F089976600300015015&amp;rft.au=Felix+A.+Gers&amp;rft.au=J%C3%BCrgen+Schmidhuber&amp;rft.au=Fred+Cummins&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1406.1078" rel="nofollow">1406.1078</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Learning+Phrase+Representations+using+RNN+Encoder-Decoder+for+Statistical+Machine+Translation&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1406.1078&amp;rft.aulast=Cho&amp;rft.aufirst=Kyunghyun&amp;rft.au=van+Merrienboer%2C+Bart&amp;rft.au=Gulcehre%2C+Caglar&amp;rft.au=Bahdanau%2C+Dzmitry&amp;rft.au=Bougares%2C+Fethi&amp;rft.au=Schwenk%2C+Holger&amp;rft.au=Bengio%2C+Yoshua&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation web"><a class="external text" href="http://www.mattmahoney.net/dc/text.html#1218" rel="nofollow">"The Large Text Compression Benchmark"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-01-13</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Large+Text+Compression+Benchmark&amp;rft_id=http%3A%2F%2Fwww.mattmahoney.net%2Fdc%2Ftext.html%231218&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, A.; Liwicki, M.; Fernández, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (May 2009). "A Novel Connectionist System for Unconstrained Handwriting Recognition". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502" rel="nofollow">10.1.1.139.4502</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Ftpami.2008.137" rel="nofollow">10.1109/tpami.2008.137</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/0162-8828" rel="nofollow">0162-8828</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19299860" rel="nofollow">19299860</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=A+Novel+Connectionist+System+for+Unconstrained+Handwriting+Recognition&amp;rft.volume=31&amp;rft.issue=5&amp;rft.pages=855-868&amp;rft.date=2009-05&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&amp;rft.issn=0162-8828&amp;rft_id=info%3Apmid%2F19299860&amp;rft_id=info%3Adoi%2F10.1109%2Ftpami.2008.137&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Liwicki%2C+M.&amp;rft.au=Fern%C3%A1ndez%2C+S.&amp;rft.au=Bertolami%2C+R.&amp;rft.au=Bunke%2C+H.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013-03-22). "Speech Recognition with Deep Recurrent Neural Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1303.5778" rel="nofollow">1303.5778</a></span> [<a class="external text" href="//arxiv.org/archive/cs.NE" rel="nofollow">cs.NE</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Speech+Recognition+with+Deep+Recurrent+Neural+Networks&amp;rft.date=2013-03-22&amp;rft_id=info%3Aarxiv%2F1303.5778&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Mohamed%2C+Abdel-rahman&amp;rft.au=Hinton%2C+Geoffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Metz, Cade (2016-06-14). <a class="external text" href="https://www.wired.com/2016/06/apple-bringing-ai-revolution-iphone/" rel="nofollow">"Apple is bringing the AI revolution to your iphone"</a>. <i>WIRED</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2016-06-16</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=WIRED&amp;rft.atitle=Apple+is+bringing+the+AI+revolution+to+your+iphone.&amp;rft.date=2016-06-14&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2016%2F06%2Fapple-bringing-ai-revolution-iphone%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Beau15-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-Beau15_14-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Beaufays, Françoise (August 11, 2015). <a class="external text" href="http://googleresearch.blogspot.co.at/2015/08/the-neural-networks-behind-google-voice.html" rel="nofollow">"The neural networks behind Google Voice transcription"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Research+Blog&amp;rft.atitle=The+neural+networks+behind+Google+Voice+transcription&amp;rft.date=2015-08-11&amp;rft.aulast=Beaufays&amp;rft.aufirst=Fran%C3%A7oise&amp;rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.at%2F2015%2F08%2Fthe-neural-networks-behind-google-voice.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleVoiceSearch-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleVoiceSearch_15-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 24, 2015). <a class="external text" href="http://googleresearch.blogspot.co.uk/2015/09/google-voice-search-faster-and-more.html" rel="nofollow">"Google voice search: faster and more accurate"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Research+Blog&amp;rft.atitle=Google+voice+search%3A+faster+and+more+accurate&amp;rft.date=2015-09-24&amp;rft.aulast=Sak&amp;rft.aufirst=Ha%C5%9Fim&amp;rft.au=Senior%2C+Andrew&amp;rft.au=Rao%2C+Kanishka&amp;rft.au=Beaufays%2C+Fran%C3%A7oise&amp;rft.au=Schalkwyk%2C+Johan&amp;rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.uk%2F2015%2F09%2Fgoogle-voice-search-faster-and-more.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleAllo-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleAllo_16-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Khaitan, Pranav (May 18, 2016). <a class="external text" href="http://googleresearch.blogspot.co.at/2016/05/chat-smarter-with-allo.html" rel="nofollow">"Chat Smarter with Allo"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Research+Blog&amp;rft.atitle=Chat+Smarter+with+Allo&amp;rft.date=2016-05-18&amp;rft.aulast=Khaitan&amp;rft.aufirst=Pranav&amp;rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.at%2F2016%2F05%2Fchat-smarter-with-allo.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleTranslate-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleTranslate_17-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin (2016-09-26). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1609.08144" rel="nofollow">1609.08144</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Google%27s+Neural+Machine+Translation+System%3A+Bridging+the+Gap+between+Human+and+Machine+Translation&amp;rft.date=2016-09-26&amp;rft_id=info%3Aarxiv%2F1609.08144&amp;rft.aulast=Wu&amp;rft.aufirst=Yonghui&amp;rft.au=Schuster%2C+Mike&amp;rft.au=Chen%2C+Zhifeng&amp;rft.au=Le%2C+Quoc+V.&amp;rft.au=Norouzi%2C+Mohammad&amp;rft.au=Macherey%2C+Wolfgang&amp;rft.au=Krikun%2C+Maxim&amp;rft.au=Cao%2C+Yuan&amp;rft.au=Gao%2C+Qin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-WiredGoogleTranslate-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-WiredGoogleTranslate_18-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Metz, Cade (September 27, 2016). <a class="external text" href="https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/" rel="nofollow">"An Infusion of AI Makes Google Translate More Powerful Than Ever | WIRED"</a>. <i>Wired</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=An+Infusion+of+AI+Makes+Google+Translate+More+Powerful+Than+Ever+%7C+WIRED&amp;rft.date=2016-09-27&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2016%2F09%2Fgoogle-claims-ai-breakthrough-machine-translation%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleQuicktype-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleQuicktype_19-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Efrati, Amir (June 13, 2016). <a class="external text" href="https://www.theinformation.com/apples-machines-can-learn-too" rel="nofollow">"Apple's Machines Can Learn Too"</a>. <i>The Information</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Information&amp;rft.atitle=Apple%27s+Machines+Can+Learn+Too&amp;rft.date=2016-06-13&amp;rft.aulast=Efrati&amp;rft.aufirst=Amir&amp;rft_id=https%3A%2F%2Fwww.theinformation.com%2Fapples-machines-can-learn-too&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleQuicktype2-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleQuicktype2_20-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Ranger, Steve (June 14, 2016). <a class="external text" href="https://www.zdnet.com/article/ai-big-data-and-the-iphone-heres-how-apple-plans-to-protect-your-privacy" rel="nofollow">"iPhone, AI and big data: Here's how Apple plans to protect your privacy | ZDNet"</a>. <i>ZDNet</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ZDNet&amp;rft.atitle=iPhone%2C+AI+and+big+data%3A+Here%27s+how+Apple+plans+to+protect+your+privacy+%7C+ZDNet&amp;rft.date=2016-06-14&amp;rft.aulast=Ranger&amp;rft.aufirst=Steve&amp;rft_id=http%3A%2F%2Fwww.zdnet.com%2Farticle%2Fai-big-data-and-the-iphone-heres-how-apple-plans-to-protect-your-privacy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleSiri-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleSiri_21-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Smith, Chris (2016-06-13). <a class="external text" href="http://bgr.com/2016/06/13/ios-10-siri-third-party-apps/" rel="nofollow">"iOS 10: Siri now works in third-party apps, comes with extra AI features"</a>. <i>BGR</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BGR&amp;rft.atitle=iOS+10%3A+Siri+now+works+in+third-party+apps%2C+comes+with+extra+AI+features&amp;rft.date=2016-06-13&amp;rft.aulast=Smith&amp;rft.aufirst=Chris&amp;rft_id=http%3A%2F%2Fbgr.com%2F2016%2F06%2F13%2Fios-10-siri-third-party-apps%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AmazonAlexa-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-AmazonAlexa_22-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Vogels, Werner (30 November 2016). <a class="external text" href="http://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html" rel="nofollow">"Bringing the Magic of Amazon AI and Alexa to Apps on AWS. - All Things Distributed"</a>. <i>www.allthingsdistributed.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.allthingsdistributed.com&amp;rft.atitle=Bringing+the+Magic+of+Amazon+AI+and+Alexa+to+Apps+on+AWS.+-+All+Things+Distributed&amp;rft.date=2016-11-30&amp;rft.aulast=Vogels&amp;rft.aufirst=Werner&amp;rft_id=http%3A%2F%2Fwww.allthingsdistributed.com%2F2016%2F11%2Famazon-ai-and-alexa-for-all-aws-apps.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-FacebookTranslate-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-FacebookTranslate_23-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Ong, Thuy (4 August 2017). <a class="external text" href="https://www.theverge.com/2017/8/4/16093872/facebook-ai-translations-artificial-intelligence" rel="nofollow">"Facebook's translations are now powered completely by AI"</a>. <i>www.allthingsdistributed.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-02-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.allthingsdistributed.com&amp;rft.atitle=Facebook%27s+translations+are+now+powered+completely+by+AI&amp;rft.date=2017-08-04&amp;rft.aulast=Ong&amp;rft.aufirst=Thuy&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2017%2F8%2F4%2F16093872%2Ffacebook-ai-translations-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation web"><a class="external text" href="http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf" rel="nofollow">"Patient Subtyping via Time-Aware LSTM Networks"</a> <span class="cs1-format">(PDF)</span>. <i>msu.edu</i><span class="reference-accessdate">. Retrieved <span class="nowrap">21 Nov</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=msu.edu&amp;rft.atitle=Patient+Subtyping+via+Time-Aware+LSTM+Networks&amp;rft_id=http%3A%2F%2Fbiometrics.cse.msu.edu%2FPublications%2FMachineLearning%2FBaytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation web"><a class="external text" href="http://www.kdd.org/kdd2017/papers/view/patient-subtyping-via-time-aware-lstm-networks" rel="nofollow">"Patient Subtyping via Time-Aware LSTM Networks"</a>. <i>Kdd.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">24 May</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Kdd.org&amp;rft.atitle=Patient+Subtyping+via+Time-Aware+LSTM+Networks&amp;rft_id=http%3A%2F%2Fwww.kdd.org%2Fkdd2017%2Fpapers%2Fview%2Fpatient-subtyping-via-time-aware-lstm-networks&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation web"><a class="external text" href="http://www.kdd.org" rel="nofollow">"SIGKDD"</a>. <i>Kdd.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">24 May</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Kdd.org&amp;rft.atitle=SIGKDD&amp;rft_id=http%3A%2F%2Fwww.kdd.org&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation web">Haridy, Rich (August 21, 2017). <a class="external text" href="http://newatlas.com/microsoft-speech-recognition-equals-humans/50999" rel="nofollow">"Microsoft's speech recognition system is now as good as a human"</a>. <i>newatlas.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=newatlas.com&amp;rft.atitle=Microsoft%27s+speech+recognition+system+is+now+as+good+as+a+human&amp;rft.date=2017-08-21&amp;rft.aulast=Haridy&amp;rft.aufirst=Rich&amp;rft_id=http%3A%2F%2Fnewatlas.com%2Fmicrosoft-speech-recognition-equals-humans%2F50999&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation web">bro, n. <a class="external text" href="https://stats.stackexchange.com/q/320919/82135" rel="nofollow">"Why can RNNs with LSTM units also suffer from "exploding gradients"?"</a>. <i>Cross Validated</i><span class="reference-accessdate">. Retrieved <span class="nowrap">25 December</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Cross+Validated&amp;rft.atitle=Why+can+RNNs+with+LSTM+units+also+suffer+from+%22exploding+gradients%22%3F&amp;rft.aulast=bro&amp;rft.aufirst=n&amp;rft_id=https%3A%2F%2Fstats.stackexchange.com%2Fq%2F320919%2F82135&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-peepholeLSTM-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-peepholeLSTM_29-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-peepholeLSTM_29-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-peepholeLSTM_29-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Gers, F. A.; Schmidhuber, J. (2001). <a class="external text" href="ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf" rel="nofollow">"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Neural Networks</i>. <b>12</b> (6): 1333–1340. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2F72.963769" rel="nofollow">10.1109/72.963769</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/18249962" rel="nofollow">18249962</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks&amp;rft.atitle=LSTM+Recurrent+Networks+Learn+Simple+Context+Free+and+Context+Sensitive+Languages&amp;rft.volume=12&amp;rft.issue=6&amp;rft.pages=1333-1340&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1109%2F72.963769&amp;rft_id=info%3Apmid%2F18249962&amp;rft.aulast=Gers&amp;rft.aufirst=F.+A.&amp;rft.au=Schmidhuber%2C+J.&amp;rft_id=ftp%3A%2F%2Fftp.idsia.ch%2Fpub%2Fjuergen%2FL-IEEE.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-peephole2002-30"><span class="mw-cite-backlink">^ <a href="#cite_ref-peephole2002_30-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-peephole2002_30-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-peephole2002_30-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). <a class="external text" href="http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf" rel="nofollow">"Learning precise timing with LSTM recurrent networks"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>3</b>: 115–143.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Learning+precise+timing+with+LSTM+recurrent+networks&amp;rft.volume=3&amp;rft.pages=115-143&amp;rft.date=2002&amp;rft.aulast=Gers&amp;rft.aufirst=F.&amp;rft.au=Schraudolph%2C+N.&amp;rft.au=Schmidhuber%2C+J.&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fgers02a%2Fgers02a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation journal">Gers, F. A.; Schmidhuber, E. (November 2001). <a class="external text" href="ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf" rel="nofollow">"LSTM recurrent networks learn simple context-free and context-sensitive languages"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Neural Networks</i>. <b>12</b> (6): 1333–1340. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2F72.963769" rel="nofollow">10.1109/72.963769</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/1045-9227" rel="nofollow">1045-9227</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/18249962" rel="nofollow">18249962</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks&amp;rft.atitle=LSTM+recurrent+networks+learn+simple+context-free+and+context-sensitive+languages&amp;rft.volume=12&amp;rft.issue=6&amp;rft.pages=1333-1340&amp;rft.date=2001-11&amp;rft.issn=1045-9227&amp;rft_id=info%3Apmid%2F18249962&amp;rft_id=info%3Adoi%2F10.1109%2F72.963769&amp;rft.aulast=Gers&amp;rft.aufirst=F.+A.&amp;rft.au=Schmidhuber%2C+E.&amp;rft_id=ftp%3A%2F%2Fftp.idsia.ch%2Fpub%2Fjuergen%2FL-IEEE.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation journal">Xingjian Shi; Zhourong Chen; Hao Wang; Dit-Yan Yeung; Wai-kin Wong; Wang-chun Woo (2015). "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting". <i>Proceedings of the 28th International Conference on Neural Information Processing Systems</i>: 802–810. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1506.04214" rel="nofollow">1506.04214</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv150604214S" rel="nofollow">2015arXiv150604214S</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+28th+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.atitle=Convolutional+LSTM+Network%3A+A+Machine+Learning+Approach+for+Precipitation+Nowcasting&amp;rft.pages=802-810&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1506.04214&amp;rft_id=info%3Abibcode%2F2015arXiv150604214S&amp;rft.au=Xingjian+Shi&amp;rft.au=Zhourong+Chen&amp;rft.au=Hao+Wang&amp;rft.au=Dit-Yan+Yeung&amp;rft.au=Wai-kin+Wong&amp;rft.au=Wang-chun+Woo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text">S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991.</span>
</li>
<li id="cite_note-gradf-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-gradf_34-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Hochreiter, S.; Bengio, Y.; Frasconi, P.; Schmidhuber, J. (2001). <a class="external text" href="https://www.researchgate.net/publication/2839938" rel="nofollow">"Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies (PDF Download Available)"</a>.  In Kremer and, S. C.; Kolen, J. F. (eds.). <i>A Field Guide to Dynamical Recurrent Neural Networks</i>. IEEE Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Gradient+Flow+in+Recurrent+Nets%3A+the+Difficulty+of+Learning+Long-Term+Dependencies+%28PDF+Download+Available%29&amp;rft.btitle=A+Field+Guide+to+Dynamical+Recurrent+Neural+Networks.&amp;rft.pub=IEEE+Press&amp;rft.date=2001&amp;rft.aulast=Hochreiter&amp;rft.aufirst=S.&amp;rft.au=Bengio%2C+Y.&amp;rft.au=Frasconi%2C+P.&amp;rft.au=Schmidhuber%2C+J.&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F2839938&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-fernandez2007-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-fernandez2007_35-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "Sequence labelling in structured domains with hierarchical recurrent neural networks". <i>Proc. 20th Int. Joint Conf. On Artificial Intelligence, Ijcai 2007</i>: 774–779. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.1887" rel="nofollow">10.1.1.79.1887</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+20th+Int.+Joint+Conf.+On+Artificial+Intelligence%2C+Ijcai+2007&amp;rft.atitle=Sequence+labelling+in+structured+domains+with+hierarchical+recurrent+neural+networks&amp;rft.pages=774-779&amp;rft.date=2007&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.79.1887&amp;rft.aulast=Fern%C3%A1ndez&amp;rft.aufirst=Santiago&amp;rft.au=Graves%2C+Alex&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-graves2006-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-graves2006_36-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". <i>In Proceedings of the International Conference on Machine Learning, ICML 2006</i>: 369–376. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.6306" rel="nofollow">10.1.1.75.6306</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=In+Proceedings+of+the+International+Conference+on+Machine+Learning%2C+ICML+2006&amp;rft.atitle=Connectionist+temporal+classification%3A+Labelling+unsegmented+sequence+data+with+recurrent+neural+networks&amp;rft.pages=369-376&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.75.6306&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Fern%C3%A1ndez%2C+Santiago&amp;rft.au=Gomez%2C+Faustino&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-wierstra2005-37"><span class="mw-cite-backlink">^ <a href="#cite_ref-wierstra2005_37-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-wierstra2005_37-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Wierstra, Daan; Schmidhuber, J.; Gomez, F. J. (2005). <a class="external text" href="https://www.academia.edu/5830256" rel="nofollow">"Evolino: Hybrid Neuroevolution/Optimal Linear Search for Sequence Learning"</a>. <i>Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), Edinburgh</i>: 853–858.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+19th+International+Joint+Conference+on+Artificial+Intelligence+%28IJCAI%29%2C+Edinburgh&amp;rft.atitle=Evolino%3A+Hybrid+Neuroevolution%2FOptimal+Linear+Search+for+Sequence+Learning&amp;rft.pages=853-858&amp;rft.date=2005&amp;rft.aulast=Wierstra&amp;rft.aufirst=Daan&amp;rft.au=Schmidhuber%2C+J.&amp;rft.au=Gomez%2C+F.+J.&amp;rft_id=https%3A%2F%2Fwww.academia.edu%2F5830256&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-OpenAIfive-38"><span class="mw-cite-backlink">^ <a href="#cite_ref-OpenAIfive_38-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-OpenAIfive_38-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Rodriguez, Jesus (July 2, 2018). <a class="external text" href="https://towardsdatascience.com/the-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69" rel="nofollow">"The Science Behind OpenAI Five that just Produced One of the Greatest Breakthrough in the History of AI"</a>. <i>Towards Data Science</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=The+Science+Behind+OpenAI+Five+that+just+Produced+One+of+the+Greatest+Breakthrough+in+the+History+of+AI&amp;rft.date=2018-07-02&amp;rft.aulast=Rodriguez&amp;rft.aufirst=Jesus&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fthe-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-OpenAIhand-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-OpenAIhand_39-0">^</a></b></span> <span class="reference-text"><cite class="citation news"><a class="external text" href="https://blog.openai.com/learning-dexterity/" rel="nofollow">"Learning Dexterity"</a>. <i>OpenAI Blog</i>. July 30, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=OpenAI+Blog&amp;rft.atitle=Learning+Dexterity&amp;rft.date=2018-07-30&amp;rft_id=https%3A%2F%2Fblog.openai.com%2Flearning-dexterity%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-alphastar-40"><span class="mw-cite-backlink">^ <a href="#cite_ref-alphastar_40-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-alphastar_40-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Stanford, Stacy (January 25, 2019). <a class="external text" href="https://medium.com/mlmemoirs/deepminds-ai-alphastar-showcases-significant-progress-towards-agi-93810c94fbe9" rel="nofollow">"DeepMind's AI, AlphaStar Showcases Significant Progress Towards AGI"</a>. <i>Medium ML Memoirs</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Medium+ML+Memoirs&amp;rft.atitle=DeepMind%27s+AI%2C+AlphaStar+Showcases+Significant+Progress+Towards+AGI&amp;rft.date=2019-01-25&amp;rft.aulast=Stanford&amp;rft.aufirst=Stacy&amp;rft_id=https%3A%2F%2Fmedium.com%2Fmlmemoirs%2Fdeepminds-ai-alphastar-showcases-significant-progress-towards-agi-93810c94fbe9&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><cite class="citation book">Mayer, H.; Gomez, F.; Wierstra, D.; Nagy, I.; Knoll, A.; Schmidhuber, J. (October 2006). <i>A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks</i>. <i>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</i>. pp. 543–548. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.218.3399" rel="nofollow">10.1.1.218.3399</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2FIROS.2006.282190" rel="nofollow">10.1109/IROS.2006.282190</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4244-0258-8" title="Special:BookSources/978-1-4244-0258-8"><bdi>978-1-4244-0258-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+System+for+Robotic+Heart+Surgery+that+Learns+to+Tie+Knots+Using+Recurrent+Neural+Networks&amp;rft.pages=543-548&amp;rft.date=2006-10&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.218.3399&amp;rft_id=info%3Adoi%2F10.1109%2FIROS.2006.282190&amp;rft.isbn=978-1-4244-0258-8&amp;rft.aulast=Mayer&amp;rft.aufirst=H.&amp;rft.au=Gomez%2C+F.&amp;rft.au=Wierstra%2C+D.&amp;rft.au=Nagy%2C+I.&amp;rft.au=Knoll%2C+A.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, A.; Schmidhuber, J. (2005). "Framewise phoneme classification with bidirectional LSTM and other neural network architectures". <i>Neural Networks</i>. <b>18</b> (5–6): 602–610. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.5800" rel="nofollow">10.1.1.331.5800</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1016%2Fj.neunet.2005.06.042" rel="nofollow">10.1016/j.neunet.2005.06.042</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16112549" rel="nofollow">16112549</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Framewise+phoneme+classification+with+bidirectional+LSTM+and+other+neural+network+architectures&amp;rft.volume=18&amp;rft.issue=5%E2%80%936&amp;rft.pages=602-610&amp;rft.date=2005&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.331.5800&amp;rft_id=info%3Apmid%2F16112549&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2005.06.042&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><cite class="citation book">Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). <a class="external text" href="http://dl.acm.org/citation.cfm?id=1778066.1778092" rel="nofollow"><i>An Application of Recurrent Neural Networks to Discriminative Keyword Spotting</i></a>. <i>Proceedings of the 17th International Conference on Artificial Neural Networks</i>. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3540746935" title="Special:BookSources/978-3540746935"><bdi>978-3540746935</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+Application+of+Recurrent+Neural+Networks+to+Discriminative+Keyword+Spotting&amp;rft.place=Berlin%2C+Heidelberg&amp;rft.series=ICANN%2707&amp;rft.pages=220-229&amp;rft.pub=Springer-Verlag&amp;rft.date=2007&amp;rft.isbn=978-3540746935&amp;rft.aulast=Fern%C3%A1ndez&amp;rft.aufirst=Santiago&amp;rft.au=Graves%2C+Alex&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1778066.1778092&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ReferenceA-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-ReferenceA_44-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). "Speech Recognition with Deep Recurrent Neural Networks". <i>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</i>: 6645–6649.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Acoustics%2C+Speech+and+Signal+Processing+%28ICASSP%29%2C+2013+IEEE+International+Conference+on&amp;rft.atitle=Speech+Recognition+with+Deep+Recurrent+Neural+Networks&amp;rft.pages=6645-6649&amp;rft.date=2013&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Mohamed%2C+Abdel-rahman&amp;rft.au=Hinton%2C+Geoffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><cite class="citation book">Eck, Douglas; Schmidhuber, Jürgen (2002-08-28). <i>Learning the Long-Term Structure of the Blues</i>. <i>Artificial Neural Networks — ICANN 2002</i>. Lecture Notes in Computer Science. <b>2415</b>. Springer, Berlin, Heidelberg. pp. 284–289. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.3620" rel="nofollow">10.1.1.116.3620</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F3-540-46084-5_47" rel="nofollow">10.1007/3-540-46084-5_47</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3540460848" title="Special:BookSources/978-3540460848"><bdi>978-3540460848</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Learning+the+Long-Term+Structure+of+the+Blues&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=284-289&amp;rft.pub=Springer%2C+Berlin%2C+Heidelberg&amp;rft.date=2002-08-28&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.116.3620&amp;rft_id=info%3Adoi%2F10.1007%2F3-540-46084-5_47&amp;rft.isbn=978-3540460848&amp;rft.aulast=Eck&amp;rft.aufirst=Douglas&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schmidhuber, J.; Gers, F.; Eck, D.; Schmidhuber, J.; Gers, F. (2002). "Learning nonregular languages: A comparison of simple recurrent networks and LSTM". <i>Neural Computation</i>. <b>14</b> (9): 2039–2041. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.7369" rel="nofollow">10.1.1.11.7369</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1162%2F089976602320263980" rel="nofollow">10.1162/089976602320263980</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12184841" rel="nofollow">12184841</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Learning+nonregular+languages%3A+A+comparison+of+simple+recurrent+networks+and+LSTM&amp;rft.volume=14&amp;rft.issue=9&amp;rft.pages=2039-2041&amp;rft.date=2002&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.11.7369&amp;rft_id=info%3Apmid%2F12184841&amp;rft_id=info%3Adoi%2F10.1162%2F089976602320263980&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J.&amp;rft.au=Gers%2C+F.&amp;rft.au=Eck%2C+D.&amp;rft.au=Schmidhuber%2C+J.&amp;rft.au=Gers%2C+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><cite class="citation journal">Perez-Ortiz, J. A.; Gers, F. A.; Eck, D.; Schmidhuber, J. (2003). "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets". <i>Neural Networks</i>. <b>16</b> (2): 241–250. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.381.1992" rel="nofollow">10.1.1.381.1992</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1016%2Fs0893-6080%2802%2900219-8" rel="nofollow">10.1016/s0893-6080(02)00219-8</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12628609" rel="nofollow">12628609</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Kalman+filters+improve+LSTM+network+performance+in+problems+unsolvable+by+traditional+recurrent+nets&amp;rft.volume=16&amp;rft.issue=2&amp;rft.pages=241-250&amp;rft.date=2003&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.381.1992&amp;rft_id=info%3Apmid%2F12628609&amp;rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2802%2900219-8&amp;rft.aulast=Perez-Ortiz&amp;rft.aufirst=J.+A.&amp;rft.au=Gers%2C+F.+A.&amp;rft.au=Eck%2C+D.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text">A. Graves, J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks. Advances in Neural Information Processing Systems 22, NIPS'22, pp 545–552, Vancouver, MIT Press, 2009.</span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><cite class="citation book">Graves, Alex; Fernández, Santiago; Liwicki, Marcus; Bunke, Horst; Schmidhuber, Jürgen (2007). <a class="external text" href="http://dl.acm.org/citation.cfm?id=2981562.2981635" rel="nofollow"><i>Unconstrained Online Handwriting Recognition with Recurrent Neural Networks</i></a>. <i>Proceedings of the 20th International Conference on Neural Information Processing Systems</i>. NIPS'07. USA: Curran Associates Inc. pp. 577–584. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/9781605603520" title="Special:BookSources/9781605603520"><bdi>9781605603520</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Unconstrained+Online+Handwriting+Recognition+with+Recurrent+Neural+Networks&amp;rft.place=USA&amp;rft.series=NIPS%2707&amp;rft.pages=577-584&amp;rft.pub=Curran+Associates+Inc.&amp;rft.date=2007&amp;rft.isbn=9781605603520&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Fern%C3%A1ndez%2C+Santiago&amp;rft.au=Liwicki%2C+Marcus&amp;rft.au=Bunke%2C+Horst&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2981562.2981635&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text">M. Baccouche, F. Mamalet, C Wolf, C. Garcia, A. Baskurt. Sequential Deep Learning for Human Action Recognition. 2nd International Workshop on Human Behavior Understanding (HBU), A.A. Salah, B. Lepri ed. Amsterdam, Netherlands. pp. 29–39. Lecture Notes in Computer Science 7065. Springer. 2011</span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Huang, Jie; Zhou, Wengang; Zhang, Qilin; Li, Houqiang; Li, Weiping (2018-01-30). "Video-based Sign Language Recognition without Temporal Segmentation". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1801.10111" rel="nofollow">1801.10111</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CV" rel="nofollow">cs.CV</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Video-based+Sign+Language+Recognition+without+Temporal+Segmentation&amp;rft.date=2018-01-30&amp;rft_id=info%3Aarxiv%2F1801.10111&amp;rft.aulast=Huang&amp;rft.aufirst=Jie&amp;rft.au=Zhou%2C+Wengang&amp;rft.au=Zhang%2C+Qilin&amp;rft.au=Li%2C+Houqiang&amp;rft.au=Li%2C+Weiping&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hochreiter, S.; Heusel, M.; Obermayer, K. (2007). "Fast model-based protein homology detection without alignment". <i>Bioinformatics</i>. <b>23</b> (14): 1728–1736. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1093%2Fbioinformatics%2Fbtm247" rel="nofollow">10.1093/bioinformatics/btm247</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17488755" rel="nofollow">17488755</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bioinformatics&amp;rft.atitle=Fast+model-based+protein+homology+detection+without+alignment&amp;rft.volume=23&amp;rft.issue=14&amp;rft.pages=1728-1736&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1093%2Fbioinformatics%2Fbtm247&amp;rft_id=info%3Apmid%2F17488755&amp;rft.aulast=Hochreiter&amp;rft.aufirst=S.&amp;rft.au=Heusel%2C+M.&amp;rft.au=Obermayer%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><cite class="citation journal">Thireou, T.; Reczko, M. (2007). "Bidirectional Long Short-Term Memory Networks for predicting the subcellular localization of eukaryotic proteins". <i>IEEE/ACM Transactions on Computational Biology and Bioinformatics</i>. <b>4</b> (3): 441–446. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Ftcbb.2007.1015" rel="nofollow">10.1109/tcbb.2007.1015</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17666763" rel="nofollow">17666763</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE%2FACM+Transactions+on+Computational+Biology+and+Bioinformatics&amp;rft.atitle=Bidirectional+Long+Short-Term+Memory+Networks+for+predicting+the+subcellular+localization+of+eukaryotic+proteins&amp;rft.volume=4&amp;rft.issue=3&amp;rft.pages=441-446&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1109%2Ftcbb.2007.1015&amp;rft_id=info%3Apmid%2F17666763&amp;rft.aulast=Thireou&amp;rft.aufirst=T.&amp;rft.au=Reczko%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><cite class="citation journal">Malhotra, Pankaj; Vig, Lovekesh; Shroff, Gautam; Agarwal, Puneet (April 2015). <a class="external text" href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf" rel="nofollow">"Long Short Term Memory Networks for Anomaly Detection in Time Series"</a> <span class="cs1-format">(PDF)</span>. <i>European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning — ESANN 2015</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=European+Symposium+on+Artificial+Neural+Networks%2C+Computational+Intelligence+and+Machine+Learning+%E2%80%94+ESANN+2015&amp;rft.atitle=Long+Short+Term+Memory+Networks+for+Anomaly+Detection+in+Time+Series&amp;rft.date=2015-04&amp;rft.aulast=Malhotra&amp;rft.aufirst=Pankaj&amp;rft.au=Vig%2C+Lovekesh&amp;rft.au=Shroff%2C+Gautam&amp;rft.au=Agarwal%2C+Puneet&amp;rft_id=https%3A%2F%2Fwww.elen.ucl.ac.be%2FProceedings%2Fesann%2Fesannpdf%2Fes2015-56.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><cite class="citation book">Tax, N.; Verenich, I.; La Rosa, M.; Dumas, M. (2017). <i>Predictive Business Process Monitoring with LSTM neural networks</i>. <i>Proceedings of the International Conference on Advanced Information Systems Engineering (CAiSE)</i>. Lecture Notes in Computer Science. <b>10253</b>. pp. 477–492. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1612.02130" rel="nofollow">1612.02130</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-319-59536-8_30" rel="nofollow">10.1007/978-3-319-59536-8_30</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-319-59535-1" title="Special:BookSources/978-3-319-59535-1"><bdi>978-3-319-59535-1</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Predictive+Business+Process+Monitoring+with+LSTM+neural+networks&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=477-492&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1612.02130&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-59536-8_30&amp;rft.isbn=978-3-319-59535-1&amp;rft.aulast=Tax&amp;rft.aufirst=N.&amp;rft.au=Verenich%2C+I.&amp;rft.au=La+Rosa%2C+M.&amp;rft.au=Dumas%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><cite class="citation journal">Choi, E.; Bahadori, M.T.; Schuetz, E.; Stewart, W.; Sun, J. (2016). <a class="external text" href="http://proceedings.mlr.press/v56/Choi16.html" rel="nofollow">"Doctor AI: Predicting Clinical Events via Recurrent Neural Networks"</a>. <i>Proceedings of the 1st Machine Learning for Healthcare Conference</i>: 301–318. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1511.05942" rel="nofollow">1511.05942</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv151105942C" rel="nofollow">2015arXiv151105942C</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+1st+Machine+Learning+for+Healthcare+Conference&amp;rft.atitle=Doctor+AI%3A+Predicting+Clinical+Events+via+Recurrent+Neural+Networks&amp;rft.pages=301-318&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1511.05942&amp;rft_id=info%3Abibcode%2F2015arXiv151105942C&amp;rft.aulast=Choi&amp;rft.aufirst=E.&amp;rft.au=Bahadori%2C+M.T.&amp;rft.au=Schuetz%2C+E.&amp;rft.au=Stewart%2C+W.&amp;rft.au=Sun%2C+J.&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv56%2FChoi16.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text">Jia, Robin; Liang, Percy (2016-06-11). <a class="extiw" href="https://arxiv.org/abs/1606.03622" title="arxiv:1606.03622">"Data Recombination for Neural Semantic Parsing"</a>. <i>arXiv:1606.03622 [cs]</i>.</span>
</li>
<li id="cite_note-Wang_Duan_Zhang_Niu_p=1657-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-Wang_Duan_Zhang_Niu_p=1657_58-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). <a class="external text" href="https://qilin-zhang.github.io/_pages/pdfs/Segment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf" rel="nofollow">"Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation"</a> <span class="cs1-format">(PDF)</span>. <i>Sensors</i>. <b>18</b> (5): 1657. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.3390%2Fs18051657" rel="nofollow">10.3390/s18051657</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/1424-8220" rel="nofollow">1424-8220</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5982167" rel="nofollow">5982167</a></span>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/29789447" rel="nofollow">29789447</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Sensors&amp;rft.atitle=Segment-Tube%3A+Spatio-Temporal+Action+Localization+in+Untrimmed+Videos+with+Per-Frame+Segmentation&amp;rft.volume=18&amp;rft.issue=5&amp;rft.pages=1657&amp;rft.date=2018-05-22&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5982167&amp;rft.issn=1424-8220&amp;rft_id=info%3Apmid%2F29789447&amp;rft_id=info%3Adoi%2F10.3390%2Fs18051657&amp;rft.aulast=Wang&amp;rft.aufirst=Le&amp;rft.au=Duan%2C+Xuhuan&amp;rft.au=Zhang%2C+Qilin&amp;rft.au=Niu%2C+Zhenxing&amp;rft.au=Hua%2C+Gang&amp;rft.au=Zheng%2C+Nanning&amp;rft_id=https%3A%2F%2Fqilin-zhang.github.io%2F_pages%2Fpdfs%2FSegment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Duan_Wang_Zhai_Zheng_2018_p.-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-Duan_Wang_Zhai_Zheng_2018_p._59-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">Duan, Xuhuan; Wang, Le; Zhai, Changbo; Zheng, Nanning; Zhang, Qilin; Niu, Zhenxing; Hua, Gang (2018). <i>Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation</i>. 25th IEEE International Conference on Image Processing (ICIP). <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Ficip.2018.8451692" rel="nofollow">10.1109/icip.2018.8451692</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4799-7061-2" title="Special:BookSources/978-1-4799-7061-2"><bdi>978-1-4799-7061-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Joint+Spatio-Temporal+Action+Localization+in+Untrimmed+Videos+with+Per-Frame+Segmentation&amp;rft.pub=25th+IEEE+International+Conference+on+Image+Processing+%28ICIP%29&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1109%2Ficip.2018.8451692&amp;rft.isbn=978-1-4799-7061-2&amp;rft.aulast=Duan&amp;rft.aufirst=Xuhuan&amp;rft.au=Wang%2C+Le&amp;rft.au=Zhai%2C+Changbo&amp;rft.au=Zheng%2C+Nanning&amp;rft.au=Zhang%2C+Qilin&amp;rft.au=Niu%2C+Zhenxing&amp;rft.au=Hua%2C+Gang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&amp;action=edit&amp;section=17" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="external text" href="http://www.idsia.ch/~juergen/rnn.html" rel="nofollow">Recurrent Neural Networks</a> with over 30 LSTM papers by <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>'s group at <a class="mw-redirect" href="/wiki/IDSIA" title="IDSIA">IDSIA</a></li>
<li><cite class="citation web">Gers, Felix (2001). <a class="external text" href="http://www.felixgers.de/papers/phd.pdf" rel="nofollow">"Long Short-Term Memory in Recurrent Neural Networks"</a> <span class="cs1-format">(PDF)</span>. <i>PhD thesis</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=PhD+thesis&amp;rft.atitle=Long+Short-Term+Memory+in+Recurrent+Neural+Networks&amp;rft.date=2001&amp;rft.aulast=Gers&amp;rft.aufirst=Felix&amp;rft_id=http%3A%2F%2Fwww.felixgers.de%2Fpapers%2Fphd.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation journal">Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). <a class="external text" href="http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf" rel="nofollow">"Learning precise timing with LSTM recurrent networks"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>3</b>: 115–143.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Learning+precise+timing+with+LSTM+recurrent+networks&amp;rft.volume=3&amp;rft.pages=115-143&amp;rft.date=2002-08&amp;rft.aulast=Gers&amp;rft.aufirst=Felix+A.&amp;rft.au=Schraudolph%2C+Nicol+N.&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fgers02a%2Fgers02a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation web">Abidogun, Olusola Adeniyi (2005). <a class="external text" href="http://etd.uwc.ac.za/xmlui/handle/11394/249" rel="nofollow">"Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks"</a>. <i>Master's Thesis</i>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a class="external text" href="//hdl.handle.net/11394%2F249" rel="nofollow">11394/249</a>. <a class="external text" href="https://web.archive.org/web/20120522234026/http://etd.uwc.ac.za/usrfiles/modules/etd/docs/etd_init_3937_1174040706.pdf" rel="nofollow">Archived</a> <span class="cs1-format">(PDF)</span> from the original on May 22, 2012.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Master%27s+Thesis&amp;rft.atitle=Data+Mining%2C+Fraud+Detection+and+Mobile+Telecommunications%3A+Call+Pattern+Analysis+with+Unsupervised+Neural+Networks&amp;rft.date=2005&amp;rft_id=info%3Ahdl%2F11394%2F249&amp;rft.aulast=Abidogun&amp;rft.aufirst=Olusola+Adeniyi&amp;rft_id=http%3A%2F%2Fetd.uwc.ac.za%2Fxmlui%2Fhandle%2F11394%2F249&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>
<ul><li><a class="external text" href="http://etd.uwc.ac.za/bitstream/handle/11394/249/Abidogun_MSC_2005.pdf" rel="nofollow">original</a> with two chapters devoted to explaining recurrent neural networks, especially LSTM.</li></ul></li>
<li><cite class="citation web">Monner, Derek D.; Reggia, James A. (2010). <a class="external text" href="http://www.cs.umd.edu/~dmonner/papers/nn2012.pdf" rel="nofollow">"A generalized LSTM-like training algorithm for second-order recurrent neural networks"</a> <span class="cs1-format">(PDF)</span>. <q>High-performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures</q></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+generalized+LSTM-like+training+algorithm+for+second-order+recurrent+neural+networks&amp;rft.date=2010&amp;rft.aulast=Monner&amp;rft.aufirst=Derek+D.&amp;rft.au=Reggia%2C+James+A.&amp;rft_id=http%3A%2F%2Fwww.cs.umd.edu%2F~dmonner%2Fpapers%2Fnn2012.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation web">Herta, Christian. <a class="external text" href="http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.html" rel="nofollow">"How to implement LSTM in Python with Theano"</a>. <i>Tutorial</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Tutorial&amp;rft.atitle=How+to+implement+LSTM+in+Python+with+Theano&amp;rft.aulast=Herta&amp;rft.aufirst=Christian&amp;rft_id=http%3A%2F%2Fchristianherta.de%2Flehre%2FdataScience%2FmachineLearning%2FneuralNetworks%2FLSTM.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li></ul>
<!-- 
NewPP limit report
Parsed by mw1266
Cached time: 20190918204926
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.980 seconds
Real time usage: 1.221 seconds
Preprocessor visited node count: 3650/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 150494/2097152 bytes
Template argument size: 1625/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 194720/5000000 bytes
Number of Wikibase entities loaded: 7/400
Lua time usage: 0.559/10.000 seconds
Lua memory usage: 5.8 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  904.354      1 -total
 78.32%  708.291      1 Template:Reflist
 41.21%  372.723     25 Template:Cite_journal
 10.13%   91.641     15 Template:Cite_web
  7.12%   64.422      3 Template:Cite_arxiv
  6.62%   59.846      1 Template:Citation_needed
  6.06%   54.779      6 Template:Cite_book
  6.01%   54.365      1 Template:Machine_learning_bar
  5.75%   51.989      8 Template:Cite_news
  5.50%   49.772      1 Template:Sidebar_with_collapsible_lists
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:10711453-0!canonical!math=5 and timestamp 20190918204925 and revision id 916374525
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&amp;oldid=916374525">https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&amp;oldid=916374525</a>"</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_October_2017" title="Category:Articles with unsourced statements from October 2017">Articles with unsourced statements from October 2017</a></li></ul></div></div>
<div class="visualClear"></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Long+short-term+memory" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Long+short-term+memory" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li> </ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-main"><span><a accesskey="c" href="/wiki/Long_short-term_memory" title="View the content page [c]">Article</a></span></li><li id="ca-talk"><span><a accesskey="t" href="/wiki/Talk:Long_short-term_memory" rel="discussion" title="Discussion about the content page [t]">Talk</a></span></li> </ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><span><a href="/wiki/Long_short-term_memory">Read</a></span></li><li class="collapsible" id="ca-edit"><span><a accesskey="e" href="/w/index.php?title=Long_short-term_memory&amp;action=edit" title="Edit this page [e]">Edit</a></span></li><li class="collapsible" id="ca-history"><span><a accesskey="h" href="/w/index.php?title=Long_short-term_memory&amp;action=history" title="Past revisions of this page [h]">View history</a></span></li> </ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label"><span>More</span></h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/><input name="title" type="hidden" value="Special:Search"/><input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/><input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/> </div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">Navigation</h3>
<div class="body">
<ul>
<li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">Interaction</h3>
<div class="body">
<ul>
<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">Tools</h3>
<div class="body">
<ul>
<li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Long_short-term_memory" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Long_short-term_memory" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Long_short-term_memory&amp;oldid=916374525" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Long_short-term_memory&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q6673524" title="Link to connected data repository item [g]">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Long_short-term_memory&amp;id=916374525" title="Information on how to cite this page">Cite this page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">Print/export</h3>
<div class="body">
<ul>
<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Long+short-term+memory">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Long+short-term+memory&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Long_short-term_memory&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">Languages</h3>
<div class="body">
<ul>
<li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/Long_short-term_memory" hreflang="ca" lang="ca" title="Long short-term memory – Catalan">Català</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Long_short-term_memory" hreflang="de" lang="de" title="Long short-term memory – German">Deutsch</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%AD%D8%A7%D9%81%D8%B8%D9%87_%D8%B7%D9%88%D9%84%D8%A7%D9%86%DB%8C_%DA%A9%D9%88%D8%AA%D8%A7%D9%87-%D9%85%D8%AF%D8%AA" hreflang="fa" lang="fa" title="حافظه طولانی کوتاه-مدت – Persian">فارسی</a></li><li class="interlanguage-link interwiki-lv"><a class="interlanguage-link-target" href="https://lv.wikipedia.org/wiki/LSTM" hreflang="lv" lang="lv" title="LSTM – Latvian">Latviešu</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E9%95%B7%E3%83%BB%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6" hreflang="ja" lang="ja" title="長・短期記憶 – Japanese">日本語</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C" hreflang="ru" lang="ru" title="Долгая краткосрочная память – Russian">Русский</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B3%D0%B0_%D0%BA%D0%BE%D1%80%D0%BE%D1%82%D0%BA%D0%BE%D1%87%D0%B0%D1%81%D0%BD%D0%B0_%D0%BF%D0%B0%D0%BC%27%D1%8F%D1%82%D1%8C" hreflang="uk" lang="uk" title="Довга короткочасна пам'ять – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6" hreflang="zh" lang="zh" title="長短期記憶 – Chinese">中文</a></li> </ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q6673524#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div> </div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 18 September 2019, at 15:53<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Long_short-term_memory&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico">
<a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a> </li>
<li id="footer-poweredbyico">
<a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a> </li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.980","walltime":"1.221","ppvisitednodes":{"value":3650,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":150494,"limit":2097152},"templateargumentsize":{"value":1625,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":8,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":194720,"limit":5000000},"entityaccesscount":{"value":7,"limit":400},"timingprofile":["100.00%  904.354      1 -total"," 78.32%  708.291      1 Template:Reflist"," 41.21%  372.723     25 Template:Cite_journal"," 10.13%   91.641     15 Template:Cite_web","  7.12%   64.422      3 Template:Cite_arxiv","  6.62%   59.846      1 Template:Citation_needed","  6.06%   54.779      6 Template:Cite_book","  6.01%   54.365      1 Template:Machine_learning_bar","  5.75%   51.989      8 Template:Cite_news","  5.50%   49.772      1 Template:Sidebar_with_collapsible_lists"]},"scribunto":{"limitreport-timeusage":{"value":"0.559","limit":"10.000"},"limitreport-memusage":{"value":6077232,"limit":52428800}},"cachereport":{"origin":"mw1266","timestamp":"20190918204926","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Long short-term memory","url":"https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory","sameAs":"http:\/\/www.wikidata.org\/entity\/Q6673524","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q6673524","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2007-04-16T20:18:38Z","dateModified":"2019-09-18T15:53:04Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3b\/The_LSTM_cell.png","headline":"recurrent neural network architecture"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":130,"wgHostname":"mw1326"});});</script>
</body>
</html>
